# Training
batch_size_per_gpu: 16
gradient_accumulation_steps: 2
n_updates: 50000
learning_rate: 0.0004
log_interval: 2500

# these can vary but for convenience, here
embed_dim: 32
num_heads: 8
hidden_dim: 128
num_blocks: 4

# resume from checkpoint
checkpoint: null
